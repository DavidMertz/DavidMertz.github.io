<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Title -->
    <title>Cleaning Data for Effective Data Science</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="assets/css/bootstrap.min.css">
    <!-- Font awesome CSS -->
    <link rel="stylesheet" href="assets/css/font-awesome.min.css">
    <!-- Animate CSS -->
    <link rel="stylesheet" href="assets/css/animate.min.css">
    <!-- Main CSS -->
    <link rel="stylesheet" href="style.css">
    <!-- Responsive CSS -->
    <link rel="stylesheet" href="assets/css/responsive.css">
    <!-- jQuery -->
    <script src="assets/js/jquery-1.11.3.min.js"></script>
    <!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML-full,Safe">
    </script>
</head>

<body>
    <!--left bar area-->
    <div class="left-bar-area">
      <div class="logo">
          <a href="index.html">
              <img src="img/home.png" alt="Home" width="50px" /></a>
          <div class="menu-switch">
              <span class="menu-bar-one"></span>
              <span class="menu-bar-two"></span>
              <span class="menu-bar-three"></span>
          </div>
      </div>

      <div class="social">
        <a href="cleaning-data.pdf"><img src="img/book.png" alt="PDF"></a>
        <a href="https://gnosis.cx/publish/">
          <img src="img/DQM-Mohawk-head.jpg" alt="David Mertz Publications"></a>
        <a href="https://www.packtpub.com/product/cleaning-data-for-effective-data-science/9781801071291">
          <img src="img/packt-squarelogo.png" alt="Buy from Packt" /></a>
        <a href="https://www.amazon.com/Cleaning-Data-Effective-Science-command-line/dp/1801071292">
          <img src="https://i.pinimg.com/originals/29/f6/41/29f641b507ccb60bb7a62a830b988736.jpg"
               alt="Buy from Amazon" /></a>
        </div>
    </div>

    <!--Nav Menu Start -->
    <nav class="mainmenu">
        <ul>
            <li><a href="preface.html">Preface</a></li>
            <li><a href="tabular.html">Tabular Formats</a></li>
            <li><a href="hierarchical.html">Hierarchical Formats</a></li>
            <li><a href="repurposing.html">Repurposing Data Sources</a></li>
            <li><a href="anomaly.html">Anomaly Detection</a></li>
            <li><a href="data-quality.html">Data Quality</a></li>
            <li><a href="value-imputation.html">Value Imputation</a></li>
            <li><a href="feature-engineering.html">Feature Engineering</a></li>
            <li><a href="closure.html">Closure</a></li>
            <li><a href="glossary.html">Glossary</a></li>
        </ul>
    </nav>

    <!--Services Area Start-->
    <nav class="navigation" id="mainNav">
        <a class="navigation__link" onclick="bookText('1')">Doing the Other 80% of the Work</a>
        <a class="navigation__link" onclick="bookText('2')">Types of Grime</a>
        <a class="navigation__link" onclick="bookText('3')">Nomenclature</a>
        <a class="navigation__link" onclick="bookText('4')">Typography</a>
        <a class="navigation__link" onclick="bookText('5')">Taxonomy</a>
        <a class="navigation__link" onclick="bookText('6')">Included Code</a>
        <a class="navigation__link" onclick="bookText('7')">Running the Book</a>
        <a class="navigation__link" onclick="bookText('8')">Using this Book</a>
        <a class="navigation__link" onclick="bookText('9')">Data Hygiene</a>
    </nav>
    <!-- Service Area End -->

    <div class="container shift-right">
        <div class="row">
            <div class="col-md-9" id="book-title"></div>
            <div class="col-md-9">
                <h1>Preface</h1>
            </div>
        </div>
        <div class="row">
            <div class="col-md-9">
                <blockquote>
                    In order for something to become clean, something else must become dirty.
                    <br />–Imbesi's Law of the Conservation of Filth
                </blockquote>
            </div>
            <div class="col-md-9">
                <div class="hidden" id="book-text" />
            </div>
            <footer>
                <p>© 2021 Packt Publishing</p>
            </footer>
        </div>
    </div>

    <div class="hidden" id="1">
        <h2>Doing the Other 80% of the Work</h2>

        <p>It is something of a truism in data science, data analysis, or machine learning that most of the effort
            needed to
            achieve your actual purpose lies in cleaning your data. The subtitle of this work alludes to a commonly
            assigned
            percentage. A keynote speaker I listened to at a data science conference a few years ago made a
            joke—perhaps one
            already widely repeated by the time he told it—about talking with a colleague of his. The colleague
            complained of
            data cleaning taking up half of her time, in response to which the speaker expressed astonishment that
            it could be
            so little as 50%.</p>

        <p>Without worrying too much about assigning a precise percentage, in my experience working as a
            technologist and data
            scientist, I have found that the bulk of what I do is preparing my data for the statistical analyses,
            machine
            learning models, or nuanced visualizations that I would like to utilize it for. Although hopeful
            executives, or
            technical managers a bit removed from the daily work, tend to have an eternal optimism that the next set
            of data the
            organization acquires will be clean and easy to work with, I have yet to find that to be true in my
            concrete
            experience.</p>

        <p>Certainly, some data is better and some is worse. But <em>all data is dirty</em>, at least within a very
            small margin
            of error in the tally. Even data sets that have been published, carefully studied, and that are widely
            distributed
            as canonical examples for statistics textbooks or software libraries, generally have a moderate number
            of data
            integrity problems. Even after our best pre-processing, a more attainable goal should be to make our
            data <em>less
                dirty</em>; making it <em>clean</em> remains unduly utopian in aspiration.</p>

        <p>By all means we should distinguish <em>data quality</em> from <em>data utility</em>. These descriptions
            are roughly
            orthogonal to each other. Data can be dirty (up to a point) but still be enormously useful. Data can be
            (relatively)
            clean but have little purpose, or at least not be fit for purpose. Concerns about the choice of
            measurements to
            collect, or about possible selection bias, or other methodological or scientific questions are mostly
            outside the
            scope of this book. However, a fair number of techniques I present can <em>aid</em> in evaluating the
            utility of
            data, but there is often no mechanical method of remedying systemic issues. For example, statistics and
            other
            analyses may reveal—or at least strongly suggest—the unreliability of a certain data field. But the
            techniques in
            this book cannot generally automatically fix that unreliable data or collect better data.</p>

        <p>The code shown throughout this book is freely available. However, the purpose of this book is not
            learning to use the
            particular tools used for illustration, but to understand the underlying purpose of data quality. The
            concepts
            presented should be applicable in any programming language used for data processing and machine
            learning. I hope it
            will be easy to adapt the techniques I show to your own favorite collection of tools and programming
            languages.</p>
    </div>

    <div class="hidden" id="2">
        <h2>Types of Grime</h2>

        <p>There are roughly two families of problems we find in data sets. Not every problem neatly divides into
            these
            families, or at least it is not always evident which side something falls on without knowing the root
            cause. But in
            a general way we can think of structural problems in the formatting of data versus content problems in
            the actual
            values recorded. On the structural branch a format used to encode a data set might simply "put values in
            the wrong
            place" in one way or another. On the content side, the data format itself is correct, but implausible or
            wrong
            values have snuck in via flawed instruments, transcription errors, numeric overflows, or through other
            pitfalls of
            the recording process.</p>

        <p>The several early chapters that discuss "data ingestion" are much more focused on structural problems in
            data
            sources, and less on numeric or content problems. It is not always cleanly possible to separate these
            issues, but as
            a question of emphasis it makes sense for the ingestion chapters to look at structural matters, and for
            later
            chapters on anomalies, data quality, feature engineering, value imputation, and model-based cleaning to
            direct
            attention to content issues.</p>

        <p>In the case of structural problems, we almost always need manual remediation of the data. Exactly where
            the bytes
            that make up the data go wrong can vary enormously, and usually does not follow a pattern that lends
            itself to a
            single high-level description. Often we have a somewhat easier time with the content problems, but at
            the same time
            they are more likely to be irremediable even with manual work. Consider this small comma-separated value
            (CSV) data
            source, describing a 6th grade class:</p>

        <pre><code>Student#,Last Name,First Name,Favorite Color,Age
1,Johnson,Mia,periwinkle,12
2,Lopez,Liam,blue,green,13
3,Lee,Isabella,,11
4,Fisher,Mason,gray,-1
5,Gupta,Olivia,9,102
6,,Robinson,,Sophia,,blue,,12</code></pre>

        <p>In a friendly way, we have a header line that indicates reasonable field names and provides a hint as to
            the meaning
            of each column. Programmatically, we may not wish to work with the punctuation marks and spaces inside
            some field
            names, but that is a matter of tool convenience that we can address with the APIs (<em>application
                programming
                interfaces</em>; the functions and methods of a library) that data processing tools give us (perhaps
            by renaming
            them).</p>

        <p>Let us think about each record in turn. Mia Johnson, student 1, seems to have a problem-free record. Her
            row has five
            values separated by four commas, and each data value meets our intuitive expectations about the data
            type and value
            domain. The problems start hereafter.</p>

        <p>Liam Lopez has too many fields in his row. However, both columns 4 and 5 seem clearly to be in the
            lexicon of color
            names. Perhaps a duplicate entry occurred or the compound color "blue-green" was intended. Structurally
            the row has
            issues, but several plausible remediations suggest themselves.</p>

        <p>Isabella Lee is perhaps no problem at all. One of her fields is empty, meaning no favorite color is
            available. But
            structurally, this row is perfectly fine for CSV format. We will need to use some domain or problem
            knowledge to
            decide how to handle the missing value.</p>

        <p>Mason Fisher is perhaps similar to Isabella. The recorded age of -1 makes no sense in the nature of "age"
            as a data
            field, at least as we usually understand it (but maybe the encoding intends something different). On the
            other hand,
            -1 is one of several placeholder values used very commonly to represent missing data. We need to know
            our specific
            problem to know whether we can process the data with a missing age, but many times we can handle that.
            However, we
            still need to be careful not to treat the -1 as a plain value; for example, the mean, minimum, or
            standard deviation
            of ages might be thrown off by that.</p>

        <p>Olivia Gupta starts to present a trickier problem. Structurally her row looks perfect. But '9' is
            probably not a
            string in our lexicon of color names. And under our understanding of the data concerning a 6th grade
            class, we don't
            expect 102 year old students to be in it. To solve this row, we really need to know more about the
            collection
            procedure and the intention of the data. Perhaps a separate mapping of numbers to colors exists
            somewhere. Perhaps
            an age of 12 was mistranscribed as 102; but also perhaps a 102 year old serves as a teaching assistant
            in this class
            and not only students are recorded.</p>

        <p>Sophia Robinson returns us to what looks like an obvious structural error. The row, upon visual
            inspection, contains
            perfectly good and plausible values, but they are separated by duplicate commas. Somehow, persumably, a
            mechanical
            error resulted in the line being formatted wrongly. However, most high-level tools are likely to choke
            on the row in
            an uninformative way, and we will probably need to remediate the issue more manually.</p>

        <p>We have a pretty good idea what to do with these six rows of data, and even re-entering them from scratch
            would not
            be difficult. If we had a million rows instead, the difficulty would grow greatly, and would require
            considerable
            effort before we arrived at usable data.</p>
    </div>

    <div class="hidden" id="3">
        <h2>Nomenclature</h2>

        <p>In this book I will use the terms <em>feature</em>, <em>field</em>, <em>measurement</em>,
            <em>column</em>, and
            occasionally <em>variable</em> more-or-less interchangeably. Likewise, the terms <em>row</em>,
            <em>record</em>,
            <em>observation</em>, and <em>sample</em> are also near synonyms. <em>Tuple</em> is used for the same
            concept when
            discussing databases (especially academically). In different academic or business fields, different ones
            of these
            terms are more prominent; and likewise different software tools choose among these.
        </p>

        <p>Conceptually, most data can be thought of as a number of occasions on which we measure various attributes
            of a common
            underlying <em>thing</em>. In most tools, it is usually convenient to put these observations/samples
            each in a row;
            and correspondingly to store each of the measurements/features/fields pertaining to that thing in a
            column
            containing corresponding data for other comparable <em>things</em>.</p>

        <p>Inasmuch as I vary the use of these roughly equivalent terms, it is simply better to fit with the domain
            under
            discussion and to make readers familiar with all the terms, which they are likely to encounter in
            various places for
            a similar intention. The choice among near synonyms is also guided by the predominant use within the
            particular
            tool, library, or programming community that is currently being discussed.</p>

        <p>In many cases, a general concept has a strong overlap with the particular name a tool or library uses to
            implement or
            express that concept. Where relevant, I attempt to use the small typographic distinctions in the names
            to indicate
            focus. For example, I discuss <em>data frames</em> as a general paradigm for manipulating data, but
            refer to
            <em>DataFrame</em> when discussing Pandas or other libraries that use that spelling for the specific
            class used.
            Likewise, R's <em>data.frame</em> object is a specific implementation of the paradigm, and
            capitalization and
            punctuation will be adjusted for context. Similarly, in generically discussing a collection of
            associated data, I
            describe that as a <em>data set</em>; but when discussing the specific array-like object in HDF5, I use
            the spelling
            <em>dataset</em>.
        </p>
    </div>

    <div class="hidden" id="4">
        <h2>Typography</h2>

        <p>As with most programming books, code literals will be set in a <code>fixed width</code> font, whether as
            excerpts
            inline or as blocks of code between paragraphs. For example, a code snippet, often a name, will appear
            as
            <code>sklearn.pipeline.Pipeline</code>. As a block, it would appear as:
        </p>
        <pre class="input">scaler = sklearn.preprocessing.RobustScaler()
scaler.fit(X)
X_scaled = scaler.transform(X_train)
</pre>
        <p>Names of software libraries and tools will be shown in <strong>boldface</strong> on first, or early,
            mention, but
            generally in the default typeface as common nouns elsewhere. Where a name is used infrequently or
            special attention
            is drawn to its use in nearby discussion, the boldface may be repeated. <em>Italics</em> are used in
            places in the
            main text simply for emphasis of words or clauses in prose. Usually a term that is used in a special or
            distinctive
            sense within data science is italicized on first, and sometimes subsequent, use. Such terms are also
            defined in the
            Glossary.</p>

        <p>The names of software tools and libraries is a bit of a challenge to orthography (i.e. spelling).
            Capitalization, or
            lack thereof, is often used in a stylized way, and moreover sometimes these bits of software are
            rendered
            differently in different contexts. For example <strong>Python</strong> is a good proper name for a
            programming
            language, but the actual executable that launches a Python script is <code>python</code> in lower case.
            Tools or
            libraries that will usually be typed in literal form, at a command line or as a name in code, will be
            set in fixed
            width.</p>

        <p>Still other tools have both an informal and a literal name. For example <strong>scikit-learn</strong> is
            stylized in
            lowercase, but is not the actual imported name of the library, which is <code>sklearn</code>. Moreover,
            the informal
            name would look out of place when referring to subpackages such as <code>sklearn.preprocessing</code>.
            In general,
            the names of software libraries are actually pretty intuitive, but the Glossary lists the name variants
            used in
            slightly different contexts in this book.</p>
    </div>

    <div class="hidden" id="5">
        <h2>Taxonomy</h2>

        <p>Throughout this book, but especially in the first few chapters, I mention a large number of software
            tools and
            libraries that you might encounter in your work as a data scientist, developer, data analyst, or in
            another job
            title. The examples in the code of this book only use a relatively small fraction of those tools, mostly
            Python, and
            R, and a few libraries for those languages.</p>

        <p>There are a much larger number of tools which you are fairly likely to encounter, and to need to use
            during your
            work. While this book does not specifically attempt to <em>document</em> the tools themselves, not even
            those tools
            that occur in many examples, I think it is valuable for readers to understand the general role of tools
            they may
            require in their specific tasks. When mentioning tools, I try to provide a general conceptual framework
            for what
            <em>kind</em> of thing that tool is, and point in the direction of the section or chapter that discusses
            purposes
            and tools most similar to it. You most certainly do not need to be familiar with any large number of the
            tools
            mentioned—potentially with none of them at all, not even the main programming languages used in
            examples.
        </p>

        <p>The main lesson is "Don't Panic!", as Douglas Adams famously admonishes. You do not need to learn any
            specific tool
            discussed, but neither is any something you <em>cannot</em> learn when you need to or wish to. The
            Glossary of this
            book provides brief comments and definitions of terms and names used throughout this book, as well.</p>
    </div>

    <div class="hidden" id="6">
        <h2>Included Code</h2>

        <p>In this book, I will primarily use Python and associated tools, such as <strong>Pandas</strong>,
            <strong>sklearn.preprocessing</strong>, and <strong>scipy.stats</strong>, to solve the data cleaning
            problems
            presented. <strong>R</strong>, and its <strong>Tidyverse</strong> tools, will often be shown as code
            alternatives.
            Some code samples will simply use <strong>bash</strong> and the many text/data command-line processing
            tools
            available. Examples from other programming languages are occasionally mentioned, where relevant.
        </p>

        <p>Quite a few additional libraries and tools are mentioned throughout this text, either only to introduce
            them briefly
            or even only to indicate they exist. Depending on your specific workplace, codebase, and colleagues, you
            may need to
            use some or all of these, even if they are not the main tools shown in this book. The Glossary describes
            (almost)
            all libraries mentioned, with brief descriptions of their purpose.</p>

        <p>All of the code in this book is released to the Public Domain, or as Creative Commons <a
                href="https://creativecommons.org/share-your-work/public-domain/cc0/">CC0</a> if your jurisdiction
            lacks a clear
            mechanism for placing content in the Public Domain. The URL <a
                href="https://www.gnosis.cx/cleaning/">https://www.gnosis.cx/cleaning/</a> contains the code
            directly printed in
            this book, and small modules or libraries supporting the techniques demonstrated, under the same terms.
            All of the
            data sets utilized are provided at the same location. Some data sets may have different license terms,
            but only ones
            with reasonably open terms for use and modification are utilized. Because data sets are often large,
            this book will
            only reproduce directly very small data sets; I will often show a few representative sections of larger
            data in the
            text.</p>
    </div>

    <div class="hidden" id="7">
        <h2>Running the Book</h2>

        <p>This book is itself written using <a href="https://jupyter.org/">Jupyter notebooks</a>. This manner of
            creation
            allows for (almost) all the code within the book to be actively run before publication. The repository
            given above
            provides instructions and configuration files for creating a similar working environment. Code samples
            shown will
            usually be accompanied by the actual output of running them. For example, Python code:</p>

        <!-- cell -->

        <pre class="input">from src.intro_students import data, cleaned
print(data)
</pre>

        <pre>
Student#,Last Name,First Name,Favorite Color,Age
1,Johnson,Mia,periwinkle,12
2,Lopez,Liam,blue,green,13
3,Lee,Isabella,,11
4,Fisher,Mason,gray,-1
5,Gupta,Olivia,9,102
6,,Robinson,,Sophia,,blue,,12

</pre>

        <!-- cell -->

        <pre class="input">cleaned</pre>

        <table border="1">
            <thead>
                <tr style="text-align: right;">
                    <th></th>
                    <th>Last_Name</th>
                    <th>First_Name</th>
                    <th>Favorite_Color</th>
                    <th>Age</th>
                </tr>
                <tr>
                    <th>Student_No</th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <th>1</th>
                    <td>Johnson</td>
                    <td>Mia</td>
                    <td>periwinkle</td>
                    <td>12.0</td>
                </tr>
                <tr>
                    <th>2</th>
                    <td>Lopez</td>
                    <td>Liam</td>
                    <td>blue-green</td>
                    <td>13.0</td>
                </tr>
                <tr>
                    <th>3</th>
                    <td>Lee</td>
                    <td>Isabella</td>
                    <td>&lt;missing&gt;</td>
                    <td>11.0</td>
                </tr>
                <tr>
                    <th>4</th>
                    <td>Fisher</td>
                    <td>Mason</td>
                    <td>gray</td>
                    <td>NaN</td>
                </tr>
                <tr>
                    <th>5</th>
                    <td>Gupta</td>
                    <td>Olivia</td>
                    <td>sepia</td>
                    <td>NaN</td>
                </tr>
                <tr>
                    <th>6</th>
                    <td>Robinson</td>
                    <td>Sophia</td>
                    <td>blue</td>
                    <td>12.0</td>
                </tr>
            </tbody>
        </table>

        <p>Likewise in this configuration, I can run R code equally well. At times the code samples will show data
            being
            transferred between the R and Python kernels.</p>

        <!-- cell -->

        <pre class="input">%load_ext rpy2.ipython
</pre>

        <!-- cell -->

        <pre class="input">%%R -i cleaned
library(&#39;tibble&#39;)
# Select and rename columns
tibble(First=cleaned$First_Name, 
       Last=cleaned$Last_Name, 
       Age=cleaned$Age)
</pre>

        <pre># A tibble: 6 x 3
  First    Last       Age
  &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;
1 Mia      Johnson     12
2 Liam     Lopez       13
3 Isabella Lee         11
4 Mason    Fisher     NaN
5 Olivia   Gupta      NaN
6 Sophia   Robinson    12
</pre>

        <p>Command-line tools will also be shown within code cells, for example:</p>

        <!-- cell -->

        <pre class="input">%%bash
sed s/,,/,/g data/students.csv |
    cut -f2,3 -d, |
    tail -n +2 |
    tr , &#39; &#39; |
    sort
</pre>

        <pre>Fisher Mason
Gupta Olivia
Johnson Mia
Lee Isabella
Lopez Liam
Robinson Sophia
</pre>

        <p>The code in this book was run using the following versions of the main programming languages used (Python
            and R).
            Other tools like bash, shell utilities, or Scala in one section, are also used, but the first two are
            very stable
            across versions and should not vary in behavior. The large majority of the code shown will work at least
            a few
            versions back for the main languages; most likely the code will continue to work for several versions
            forward (but
            the future is unwritten). Specific libraries used, and the number touched on is numerous, may possibly
            change
            behaviors.</p>

        <!-- cell -->

        <pre class="input">import sys
sys.version
</pre>

        <pre>&#39;3.9.0 | packaged by conda-forge | (default, Oct 14 2020, 22:59:50) \n[GCC 7.5.0]&#39;</pre>

        <!-- cell -->

        <pre class="input">%%R
R.version.string
</pre>

        <pre>[1] &#34;R version 4.0.3 (2020-10-10)&#34;
</pre>
    </div>

    <div class="hidden" id="8">
        <h2>Using this Book</h2>

        <blockquote>
            <p>Slovenliness is no part of data science... cleanliness is indeed
                next to godliness.<br />–c.f. John Wesley</p>
        </blockquote>

        <p>This book is intended to be suitable for use either by a self-directed reader or in more structured
            academic,
            training, or certification courses. Each chapter is accompanied by exercises at the bottom that ask
            readers or
            students to complete tasks related to what they just learned in the preceding material. The book
            repository contains
            additional discussion of some exercises, but will avoid presenting explicit solutions for mere
            copy-paste.</p>

        <p>Instructors are encouraged to contact the author if they wish to plan course material around this book.
            Under a
            consulting arrangement, I am happy to provide solution code, suggestions on use of the exercises and
            other content,
            and so on.</p>

        <p>The data sets and supporting materials for this book are available at the repository described above, and
            will be
            needed to engage fully with some of the more open ended problems presented. These extra materials will
            allow more
            interactive use of the book, and accompanying materials, than reading only would allow. However,
            sufficient
            explanation to understand the content based on the written material only will also be provided in the
            text.</p>

        <p>Throughout this book I am <em>strongly opinionated</em> about a number of technical questions. I do not
            believe it
            will be difficult to distinguish my opinions from the <em>mere facts</em> I also present. I have worked
            in this area
            for a number of years, and I hope to share with readers the conclusions I have reached. Of course, even
            book authors
            are fallible beings, and if you decide to disagree with claims I make, I hope and wish that you will
            gain great
            benefit both from what you learn anew and what you are able to reformulate in strengthening your own
            opinions and
            conclusions.</p>

        <p>This book does not use heavy mathematics or statistics, but there are references to concepts therein from
            time to
            time. Some concepts are described briefly in the <em>Glossary</em>. Readers who want to brush up on
            these concepts
            might consider these books:</p>
        <ul>
            <li><a href="https://greenteapress.com/thinkstats2/thinkstats2.pdf"><u>Think Stats: Exploratory Data
                        Analysis in
                        Python</u></a>, Allen B. Downey, 2014 (O'Reilly Media; available both in free PDF and HTML
                versions, and
                as a printed book).</li>
            <li><u>All of Statistics: A Concise Course in Statistical Inference</u>, Larry Wasserman, 2004
                (Springer).</li>
        </ul>
        <p>This book is also not focused on the <em>ethics of data visualization</em>, but I have tried to be
            conscientious in
            using plots, which I use throughout the text. Good texts that considers these issues include:</p>
        <ul>
            <li><a href="https://socviz.co/index.html"><u>Data Visualization: A practical introduction</u></a>,
                Kieran Healy,
                2018 (Princeton University Press; a non-final draft is available free online).</li>
            <li><a href="https://www.edwardtufte.com/tufte/books_be"><u>The Visual Display of Quantitative
                        Information</u></a>,
                Edward Tufte, 2001 (Graphics Press; all four of Tufte's visualization books are canonical in the
                field).</li>
        </ul>
    </div>

    <div class="hidden" id="9">
        <h2>Data Hygiene</h2>

        <p>Throughout this book, I show you a variety of ways to modify data sets from the original versions you
            receive.
            Sometimes these transformations are between data formats or in-memory representations. At other times we
            impute,
            massage, sample, aggregate, or collate data. Every time some transformation is made on data, we bring in
            certain
            assumptions or goals of our own; these may well be—and ideally <em>should</em> be—well motivated by task
            purpose or
            numeric and statistical analysis. However, they remain assumptions that could be wrong.</p>

        <p>It is crucial to good practice of data science to <em>version</em> data sets as we work with them. When
            we draw some
            conclusion, or even simply when we prepare for the next transformation step, it is important to indicate
            which
            version of the data this action is based on. There are several different ways in which data sets may be
            versioned.
        </p>

        <p>If a data set is of moderate size, and if the transformations made are not themselves greatly time
            consuming,
            versioning within program flow is a good choice. For example, in Python-like pseudo-code:</p>
        <pre class="input">data1 = read_format(raw_data)
data2 = transformation_1(data1)
data3 = transformation_2(data2)
# ... etc ...
</pre>
        <p>When you use any version, anywhere else in a large program, it is clear from the variable name (or lookup
            key, etc.)
            which version is involved, and problems can be more easily diagnosed.</p>

        <p>If a data set is somewhat larger in size—to the point where keeping a number of near-copies in memory is
            a resource
            constraint—it is possible instead to track changes simply as metadata on the working data set. This does
            not allow
            simultaneous access to multiple versions in code, but is still very useful for debugging and analysis.
            Again, in
            pseudo-code:</p>
        <pre class="input">data = Annotated(read_format(raw_data))
inplace_transform_1(data)
data.version = &quot;Transformed by step 1&quot;
# ... actions on data ...
inplace_transform_2(data)
data.version = &quot;Transformed by step 2&quot;
# ... etc ...
</pre>
        <p>At any part of an overall program, you can at least verify the version (or other metadata) associated
            with the data
            set.</p>

        <p>For transformations that you wish to persist longer than the run of a single program, use of version
            control systems
            (VCS) is highly desirable. Most VCSs allow a concept of a <em>branch</em> where different versions of
            files can be
            maintained in parallel. If available, use of this capability is often desirable. Even if your data set
            versions are
            strictly linear, it is possible to revert to a particular earlier version if necessary. Using accurate
            and
            descriptive commit messages is a great benefit to data versioning.</p>

        <p>Most VCSs are intelligent about storing as few bytes as possible to describe changes. It is often
            possible for them
            to calculate a "minimal change set" to describe a transformation rather than simply storing an entirely
            new
            near-copy for each version. Whether or not your VCS does this with the formats you work with, data
            integrity and
            data provenance should be a more prominent concern than the potential need to allocate more disk space.
            Of late, Git
            is the most popular VCS; but the advice here can equally be followed using Apache Subversion, Mercurial,
            Perforce,
            Microsoft Visual SourceSafe, IBM Rational ClearCase, or any other modern VCS. Indeed, an older system
            like
            Concurrent Versions System (CVS) suffices for this purpose.</p>
    </div>

    <div class="hidden" id="10">
        <h2>Exercises</h2>

        <p>None of the exercises throughout this book depend on using any specific programming language. In the
            discussion,
            Python is used most frequently, followed by R, with occaional use of other programming languages. But
            all exercises
            simply present one or more data sets and ask you to perform some task with that. Achieving those goals
            using the
            programming language of your choice is wonderful (subject to any constraints your instructor may provide
            if this
            book is used in formal pedagogy).</p>

        <p>The toy tabular data on students given as an example is available at:</p>
        <blockquote>
            <p><a href="https://www.gnosis.cx/cleaning/students.csv">https://www.gnosis.cx/cleaning/students.csv</a>
            </p>
        </blockquote>
        <p>For this exercise, create a cleaned up version of the data following the assumptions illustrated in the
            code samples
            shown. Use your favorite programming language and tools, but the goal has these elements:</p>
        <ul>
            <li>Consistent doubled commas should be read as a single delimiter.</li>
            <li>Missing data in the <em>Favorite Color</em> field should be substituted with the string
                <code>&lt;missing&gt;</code>.
            </li>
            <li>Student ages should be between 9 and 14, and all other values are considered missing data. </li>
            <li>Some colors are numerically coded, but should be unaliased. The mapping is:</li>
        </ul>
        <table>
            <thead>
                <tr>
                    <th style="text-align:center">Number</th>
                    <th style="text-align:left">Color</th>
                    <th style="text-align:center">Number</th>
                    <th>Color</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="text-align:center">1</td>
                    <td style="text-align:left">beige</td>
                    <td style="text-align:center">6</td>
                    <td>alabaster</td>
                </tr>
                <tr>
                    <td style="text-align:center">2</td>
                    <td style="text-align:left">eggshell</td>
                    <td style="text-align:center">7</td>
                    <td>sandcastle</td>
                </tr>
                <tr>
                    <td style="text-align:center">3</td>
                    <td style="text-align:left">seafoam</td>
                    <td style="text-align:center">8</td>
                    <td>chartreuse</td>
                </tr>
                <tr>
                    <td style="text-align:center">4</td>
                    <td style="text-align:left">mint</td>
                    <td style="text-align:center">9</td>
                    <td>sepia</td>
                </tr>
                <tr>
                    <td style="text-align:center">5</td>
                    <td style="text-align:left">cream</td>
                    <td style="text-align:center">10</td>
                    <td>lemon</td>
                </tr>
            </tbody>
        </table>
        <p>Using the small test data set is a good way to test your code. But try also manually adding more rows
            with similar,
            or different, problems in them, and see how well your code produces a reasonable result. We have not
            discussed tools
            to accomplish this exercise yet, although you likely have used a programming language capable of solving
            it. Try to
            solve it now, but you can come back to this after later chapters if you prefer.</p>
    </div>

    <!-- Bootstrap JS -->
    <script src="assets/js/bootstrap.min.js"></script>
    <script src="assets/js/wow-1.3.0.min.js"></script>
    <script src="assets/js/waypoints.js"></script>
    <script src="assets/js/jquery.scrollUp.js"></script>
    <script src="assets/js/active.js"></script>

</body>

</html>
